@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9459--9474},
  year={2020},
  publisher={Curran Associates, Inc.}
}

@article{li2021survey,
  title={A survey on federated learning systems: vision, hype and reality for data privacy and protection},
  author={Li, Qinbin and Wen, Zeyi and Wu, Zhaomin and Hu, Sixu and Wang, Naibo and Li, Yuan and Liu, Xu and He, Bingsheng},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={35},
  number={4},
  pages={3347--3366},
  year={2021},
  publisher={IEEE}
}

@misc{cursor2024,
  title={Cursor: The AI-first Code Editor},
  author={{Cursor Team}},
  year={2024},
  url={https://cursor.com},
  note={Accessed: 2024-12-15}
}

@misc{ollama2024,
  title={Ollama: Get up and running with large language models locally},
  author={{Ollama Team}},
  year={2024},
  url={https://ollama.ai},
  note={Accessed: 2024-12-15}
}

@misc{chase2023langchain,
  title={LangChain: Building applications with LLMs through composability},
  author={Chase, Harrison},
  year={2023},
  url={https://github.com/langchain-ai/langchain},
  note={GitHub repository. Accessed: 2024-12-15}
}

@misc{chromadb2024,
  title={Chroma: The AI-native open-source embedding database},
  author={{Chroma Team}},
  year={2024},
  url={https://www.trychroma.com},
  note={Accessed: 2024-12-15}
}

@inproceedings{abid2019gradio,
  title={Gradio: Hassle-free sharing and testing of ML models in the wild},
  author={Abid, Abubakar and Abdalla, Ali and Abid, Ali and Khan, Dawood and Alfozan, Abdulrahman and Zou, James},
  booktitle={International conference on machine learning},
  pages={106--116},
  year={2019},
  organization={PMLR}
}

@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Girish, Sastry and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{team2023gemma,
  title={Gemma: Open models based on Gemini research and technology},
  author={{Gemma Team}},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@misc{aws2024ec2,
  title={Amazon Elastic Compute Cloud (EC2)},
  author={{Amazon Web Services}},
  year={2024},
  url={https://aws.amazon.com/ec2/},
  note={Accessed: 2024-12-15}
}

@misc{docker2024,
  title={Docker: Accelerated container application development},
  author={{Docker Inc.}},
  year={2024},
  url={https://www.docker.com},
  note={Accessed: 2024-12-15}
}

@inproceedings{kenton2019bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={4171--4186},
  year={2019}
}

@article{rajpurkar2016squad,
  title={SQuAD: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@article{wang2019glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@misc{huggingface2024,
  title={Hugging Face: The AI community building the future},
  author={{Hugging Face}},
  year={2024},
  url={https://huggingface.co},
  note={Accessed: 2024-12-15}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@inproceedings{krishna2024code,
  title={CodeT5+: Open code large language models for code understanding and generation},
  author={Krishna, Rohit and Wang, Yue and Zhou, Yufei and Yin, Pengcheng and Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and others},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  year={2024}
}

@misc{openondemand2024,
  title={Open OnDemand: A web-based client portal for HPC centers},
  author={{Ohio Supercomputer Center}},
  year={2024},
  url={https://openondemand.org},
  note={Accessed: 2024-12-15}
}